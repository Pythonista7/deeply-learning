{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-ggU9FfT6aGB",
        "zdnio-hC6-X_",
        "GjNiooSM_Lsy"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+F4zBe3mUdteElrSpxNyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pythonista7/deeply-learning/blob/main/Transofrmer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a transformer\n"
      ],
      "metadata": {
        "id": "_a8JSXrGQpSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s2dbk1z3Qj4Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zfL7UaASUJx",
        "outputId": "f92f300a-6ec7-4c83-d266-f1118d72b444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Head Attention\n",
        "Starting off with the core matter at hand, lets implemet attention block."
      ],
      "metadata": {
        "id": "-ggU9FfT6aGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,d_model,heads = 6):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        if d_model % heads != 0:\n",
        "          raise ValueError(\"d_model must be divisible by heads\")\n",
        "\n",
        "        self.d_head = d_model // heads\n",
        "\n",
        "        # So its really of shape [d_model, H * d_head]\n",
        "        # Think of each of these as H blocks concatenated horizontally\n",
        "        # Like WQ​=[WQ(1)​∣WQ(2)​∣…∣WQ(H)​] where each WQ(i) is of shape [d_model,d_head] stacked H times.\n",
        "        self.Q_linear_projection_layer = nn.Linear(self.d_model,self.d_model,device=device)\n",
        "        self.K_linear_projection_layer = nn.Linear(self.d_model,self.d_model,device=device)\n",
        "        self.V_linear_projection_layer = nn.Linear(self.d_model,self.d_model,device=device)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "        self.W_o = nn.Linear(d_model,d_model,device=device)\n",
        "\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "      \"\"\"\n",
        "      We assumme to get matrices of dimensions [ B, T, d_model].\n",
        "      We calculate based on number of heads the dimension of each head as d_head = d_model/H where  where H is the number of heads.\n",
        "      We use additive mask, assumming the input mask to be containing [0 or 1], 1 to preserve and 0 to hide, this can be easily generated using torch.tril()\n",
        "      \"\"\"\n",
        "      # Linear Projections\n",
        "      q_linear_projections = self.Q_linear_projection_layer(Q)\n",
        "      k_linear_projections = self.K_linear_projection_layer(K)\n",
        "      v_linear_projections = self.V_linear_projection_layer(V)\n",
        "\n",
        "      # Reshape output for heads\n",
        "      # [B,T, H * d_head]\n",
        "      B,T,D = q_linear_projections.shape\n",
        "      q_linear_projections = q_linear_projections.reshape(B,T,self.heads,self.d_head).transpose(1,2) # [B,H,T,d_head]\n",
        "      assert q_linear_projections.shape == (B,self.heads,T,self.d_head)\n",
        "\n",
        "      B,T,D = k_linear_projections.shape\n",
        "      k_linear_projections = k_linear_projections.reshape(B,T,self.heads,self.d_head).transpose(1,2) # [B,H,T,d_head]\n",
        "      assert k_linear_projections.shape == (B,self.heads,T,self.d_head)\n",
        "\n",
        "      B,T,D = v_linear_projections.shape\n",
        "      v_linear_projections = v_linear_projections.reshape(B,T,self.heads,self.d_head).transpose(1,2) # [B,H,T,d_head]\n",
        "      assert v_linear_projections.shape == (B,self.heads,T,self.d_head)\n",
        "\n",
        "      # Scaled Attention\n",
        "      scores = (q_linear_projections @ k_linear_projections.transpose(-2,-1) ) / (q_linear_projections.shape[-1] ** 0.5)\n",
        "\n",
        "      # note this might not work in case of cross attention we will need to specifically separate T_q and T_k , rn they are equal.\n",
        "      assert scores.shape == (B,self.heads,T,T)\n",
        "\n",
        "      if mask is not None:\n",
        "        # Doing this will cause the softmax -> 0 where numbers are close to -inf hence masking them\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        scores = scores + mask\n",
        "\n",
        "      scores = self.softmax(scores)\n",
        "\n",
        "      # This catches wrong softmax dim and mask weirdness instantly , basically tries to sum the dim on which softmax was applied and checks if its close to 1 which it should be.\n",
        "      assert torch.allclose(scores.sum(dim=-1), torch.ones_like(scores.sum(dim=-1)), atol=1e-4, rtol=1e-4)\n",
        "\n",
        "      scaled_attn_result = scores @ v_linear_projections\n",
        "\n",
        "      # Concat , but this is really doing the reverse of what we did in the reshape previous to scaled attention.\n",
        "      scaled_attn_result = scaled_attn_result.transpose(1,2).reshape(B,T,D)\n",
        "\n",
        "      # Output Projection\n",
        "      result = self.W_o(scaled_attn_result)\n",
        "      return result\n"
      ],
      "metadata": {
        "id": "UChE7Vd3QorR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(840)"
      ],
      "metadata": {
        "id": "J9Nzj4XPle_a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_input = torch.rand((2,10,840))"
      ],
      "metadata": {
        "id": "k4W_1fifnD0B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = mha.forward(random_input,random_input,random_input)"
      ],
      "metadata": {
        "id": "AH9nkPLEnJvs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmPRrne-n1FH",
        "outputId": "a7fce444-c634-40ab-dc8d-9217b6f9a6d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 840])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a basic attention block setup lets focus on a couple other fundamental blocks required to piece together an Encoder and a Decoder! Next we will required 2 things : 1. an embedding layer and 2. positional encoder. `1.` tells the attention block \"what\" and `2.` convey's \"where\" in the sequence, both of which are crucial."
      ],
      "metadata": {
        "id": "fnF9IspY7xwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "We need to calculate a P.E value of dims [T,d_model] as per the paper and we do a add to the input embedding, broadcasting should automatically handle adding PE to batches since PE will be same across the batch for a given position."
      ],
      "metadata": {
        "id": "zdnio-hC6-X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}} )$\n",
        "\n",
        "$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}} )$\n",
        "\n",
        "where `pos` is the position and `i` is the dimension"
      ],
      "metadata": {
        "id": "83XLlV1Gbh3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the input angles to both sin and cos are the same. so lets generate that that first and then apply sin and cos alternatively across the range."
      ],
      "metadata": {
        "id": "PxOYhqE0cBJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_encoding(T,d_model):\n",
        "  \"\"\"\n",
        "  T: sequence length\n",
        "  d_model: model dimentions , also same as embedding dims\n",
        "  \"\"\"\n",
        "  # Alot of things from this function can be precomputed ,stored and reused for better performance.\n",
        "  i = torch.arange(0,d_model,2)\n",
        "  # this is written as ^-1 so it can be multiplied insted of div\n",
        "  div = torch.exp(-torch.log(torch.tensor(10000,device=device))*i/d_model).unsqueeze(0) # (1, d_model//2)\n",
        "  pos = torch.arange(0,T,1,device=device).unsqueeze(1) # [T,1]\n",
        "  # print(f\"pos shape {pos.shape} , div shape {div.shape}\")\n",
        "  angles = pos * div\n",
        "  positional_encodings = torch.zeros((T,d_model))\n",
        "  positional_encodings[:,0::2] = torch.sin(angles)\n",
        "  positional_encodings[:,1::2] = torch.cos(angles)\n",
        "  return positional_encodings"
      ],
      "metadata": {
        "id": "X-3kve6E7Ck7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos_enc_data = torch.rand((10,840))\n",
        "get_positional_encoding(10,840)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tcSF-UmdYQU",
        "outputId": "8e707abe-f381-401f-ad3d-2b425010c659"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "          0.0000e+00,  1.0000e+00],\n",
              "        [ 8.4147e-01,  5.4030e-01,  8.2955e-01,  ...,  1.0000e+00,\n",
              "          1.0222e-04,  1.0000e+00],\n",
              "        [ 9.0930e-01, -4.1615e-01,  9.2649e-01,  ...,  1.0000e+00,\n",
              "          2.0443e-04,  1.0000e+00],\n",
              "        ...,\n",
              "        [ 6.5699e-01,  7.5390e-01,  5.3540e-01,  ...,  1.0000e+00,\n",
              "          7.1552e-04,  1.0000e+00],\n",
              "        [ 9.8936e-01, -1.4550e-01,  9.9962e-01,  ...,  1.0000e+00,\n",
              "          8.1774e-04,  1.0000e+00],\n",
              "        [ 4.1212e-01, -9.1113e-01,  5.8103e-01,  ...,  1.0000e+00,\n",
              "          9.1995e-04,  1.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a lot of data can be cache and reused instead of recomputing the positional encodings for each token, we can use `register_buffer` as a device aware cache to populate a tensor outside the computation graph to store such values."
      ],
      "metadata": {
        "id": "koczWTjaa1Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, T, d_model , **kwargs) -> None:\n",
        "    super().__init__( **kwargs)\n",
        "    self.sequence_len = T\n",
        "    self.d_model = d_model\n",
        "\n",
        "    pe = self.get_positional_encoding(self.sequence_len,self.d_model)\n",
        "    self.register_buffer('pos_enc',pe)\n",
        "\n",
        "\n",
        "  def get_positional_encoding(self,T,d_model):\n",
        "    \"\"\"\n",
        "    T: sequence length\n",
        "    d_model: model dimentions , also same as embedding dims\n",
        "    \"\"\"\n",
        "    # Alot of things from this function can be precomputed ,stored and reused for better performance.\n",
        "    i = torch.arange(0,d_model,2)\n",
        "    # this is written as ^-1 so it can be multiplied insted of div\n",
        "    div = torch.exp(-torch.log(torch.tensor(10000,device=device))*i/d_model).unsqueeze(0) # (1, d_model//2)\n",
        "    pos = torch.arange(0,T,1,device=device).unsqueeze(1) # [T,1]\n",
        "    # print(f\"pos shape {pos.shape} , div shape {div.shape}\")\n",
        "    angles = pos * div\n",
        "    positional_encodings = torch.zeros((T,d_model))\n",
        "    positional_encodings[:,0::2] = torch.sin(angles)\n",
        "    positional_encodings[:,1::2] = torch.cos(angles)\n",
        "    return positional_encodings\n",
        "\n",
        "  def forward(self,X):\n",
        "    input_seq_len = X.shape[1]\n",
        "    assert input_seq_len <= self.sequence_len , f\"Input sequence length {input_seq_len} is greater than positional encoder sequence length {self.sequence_len}\"\n",
        "    return X + self.pos_enc[:input_seq_len,:]"
      ],
      "metadata": {
        "id": "qpTiEfNtURE4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos_enc_data = torch.rand((4,10,840))"
      ],
      "metadata": {
        "id": "c90aF3r3anqY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pe = PositionalEncoder(10,840)\n",
        "test_pe(test_pos_enc_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8kMGcwjaN3I",
        "outputId": "8414f53b-72b3-429e-c553-090e6f782020"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7473,  1.3762,  0.2759,  ...,  1.4673,  0.0939,  1.8608],\n",
              "         [ 0.8826,  1.4445,  1.7402,  ...,  1.4413,  0.2259,  1.8707],\n",
              "         [ 1.0566,  0.0261,  0.9311,  ...,  1.4470,  0.8691,  1.4051],\n",
              "         ...,\n",
              "         [ 0.9384,  1.2002,  0.6811,  ...,  1.9732,  0.3912,  1.7361],\n",
              "         [ 1.9576,  0.2992,  1.6011,  ...,  1.3092,  0.0315,  1.9008],\n",
              "         [ 0.7229, -0.3882,  1.5355,  ...,  1.5789,  0.5343,  1.9023]],\n",
              "\n",
              "        [[ 0.4694,  1.9663,  0.7528,  ...,  1.9207,  0.2482,  1.0108],\n",
              "         [ 1.0322,  0.7881,  1.2624,  ...,  1.9114,  0.6086,  1.8982],\n",
              "         [ 1.0248, -0.1682,  1.7162,  ...,  1.4582,  0.9976,  1.2365],\n",
              "         ...,\n",
              "         [ 1.0778,  1.7237,  0.7035,  ...,  1.2487,  0.2576,  1.0321],\n",
              "         [ 1.0873,  0.5317,  1.6854,  ...,  1.5681,  0.5731,  1.1959],\n",
              "         [ 1.1321, -0.8077,  0.8261,  ...,  1.4930,  0.0783,  1.0357]],\n",
              "\n",
              "        [[ 0.0038,  1.4287,  0.1158,  ...,  1.8288,  0.4352,  1.6492],\n",
              "         [ 1.5294,  0.8382,  0.9557,  ...,  1.1354,  0.7669,  1.9934],\n",
              "         [ 1.3376, -0.3365,  1.5993,  ...,  1.6617,  0.7897,  1.1804],\n",
              "         ...,\n",
              "         [ 0.8301,  1.0566,  1.2368,  ...,  1.9728,  0.8930,  1.3181],\n",
              "         [ 1.8062,  0.5054,  1.3463,  ...,  1.6103,  0.6408,  1.5428],\n",
              "         [ 0.6474, -0.5708,  0.9766,  ...,  1.3241,  0.0527,  1.0420]],\n",
              "\n",
              "        [[ 0.4482,  1.6788,  0.4139,  ...,  1.6391,  0.7531,  1.0221],\n",
              "         [ 1.0275,  1.0972,  1.7268,  ...,  1.5530,  0.3231,  1.7702],\n",
              "         [ 1.8498,  0.5243,  1.1464,  ...,  1.2853,  0.1541,  1.1230],\n",
              "         ...,\n",
              "         [ 1.3925,  0.9958,  1.1942,  ...,  1.1360,  0.8539,  1.1193],\n",
              "         [ 1.6648,  0.2129,  1.0468,  ...,  1.0878,  0.4982,  1.4007],\n",
              "         [ 1.0139, -0.2071,  0.8429,  ...,  1.4210,  0.1528,  1.2554]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embedding\n",
        "What this does is take the token input and convert it into representational embeddings encoding data per token incliding both \"what the token is\" with the embedding layer and also \"where the token is\" with the positional encoding."
      ],
      "metadata": {
        "id": "GjNiooSM_Lsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,max_seq_len = 1024):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding_layer = nn.Embedding(self.vocab_size,self.d_model)\n",
        "    self.pos_encoder = PositionalEncoder(max_seq_len,self.d_model)\n",
        "\n",
        "  def forward(self,X):\n",
        "    \"\"\"\n",
        "    X: input vector of shape (batch_size,seq_len) with values in range [0,vocab_size]\n",
        "    \"\"\"\n",
        "    input_embedding = self.embedding_layer(X) * torch.sqrt(torch.tensor(self.d_model,device=device)) # scaling this by sqrt(d_model) as suggested in the paper.\n",
        "    pos_encoded_input = self.pos_encoder(input_embedding)\n",
        "    return pos_encoded_input\n"
      ],
      "metadata": {
        "id": "ZhRUdnU8_Nwi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A small test to see if this works\n",
        "B, T = 2, 5\n",
        "vocab_size = 100\n",
        "d_model = 32\n",
        "\n",
        "X = torch.randint(0, vocab_size, (B, T))   # must be long\n",
        "embed = InputEmbedding(vocab_size, d_model, max_seq_len=10)\n",
        "\n",
        "Y = embed(X)\n",
        "\n",
        "print(\"X:\", X.shape, X.dtype)\n",
        "print(\"Y:\", Y.shape, Y.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFuBPy21BwzN",
        "outputId": "baed5579-26e5-4414-b953-3fb7e2fe487b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: torch.Size([2, 5]) torch.int64\n",
            "Y: torch.Size([2, 5, 32]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n"
      ],
      "metadata": {
        "id": "zu02uZ9p6mnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size, no_of_heads,max_seq_len=1024):\n",
        "    super().__init__()\n",
        "    self.d_ff = 2048\n",
        "    # The input embedding is going to be generated outside the Encoder since we want to \"stack\" up encoders\n",
        "    # stack = feeding the output of one EncoderLayer into the next, repeatedly, with new learnable parameters each time.\n",
        "    # self.input_embedding = InputEmbedding(d_model=d_model, vocab_size=vocab_size,max_seq_len=max_seq_len)\n",
        "    self.MHA = MultiHeadAttention(d_model = d_model, heads = no_of_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.linear1 = nn.Linear(d_model,self.d_ff)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(self.d_ff,d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,X):\n",
        "    # The input is going to be of dims [B,T,d_model]\n",
        "    attn = self.MHA(Q=X,K=X,V=X)\n",
        "    layer_norm1 = self.norm1(X + attn) # Note we also add the residual skip conn here\n",
        "    feed_forward1 = self.linear1(layer_norm1)\n",
        "    a1 = self.relu1(feed_forward1)\n",
        "    feed_forward2 = self.linear2(a1)\n",
        "    layer_norm2 = self.norm2(feed_forward2 + layer_norm1) # residual skip connection\n",
        "    return layer_norm2"
      ],
      "metadata": {
        "id": "6tSKxDof6NQD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n"
      ],
      "metadata": {
        "id": "yjWGqpNIJVBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,encoder_output):\n",
        "    pass"
      ],
      "metadata": {
        "id": "SjPG1zSHJbNw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}